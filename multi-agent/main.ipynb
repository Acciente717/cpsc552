{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MultiActorEnv import MultiActorEnv\n",
    "from FCNN import FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bce159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = MultiActorEnv(actor_number=4, height=10, width=10, obs_count=5, random_seed=100)\n",
    "print(env.grid)\n",
    "env.display()\n",
    "print(\"-------------------------------------------------------------\")\n",
    "rewards, dones = env.step(['r', 'l', 'r', 'l'])\n",
    "print(env.grid)\n",
    "env.display()\n",
    "print(rewards)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "rewards, dones = env.step(['u', 'd', 'u', 'd'])\n",
    "print(env.grid)\n",
    "env.display()\n",
    "print(rewards)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "rewards, dones = env.step(['u', 'd', 'u', 'd'])\n",
    "print(env.grid)\n",
    "env.display()\n",
    "print(rewards)\n",
    "print(\"-------------------------------------------------------------\")\n",
    "rewards, dones = env.step(['d', 'u', 'd', 'u'])\n",
    "print(env.grid)\n",
    "env.display()\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.grid)\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d651e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 4 * 10 * 10 + 5\n",
    "model = FCNN(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1717c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seed(torch_seed=100, random_seed=100, np_random_seed=100):\n",
    "    torch.manual_seed(torch_seed)\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(np_random_seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.00)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def MovingAveragePlot(input_list, window_size):\n",
    "    plt.subplots(figsize=(14, 7))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(len(input_list)), input_list)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    ave_values = np.convolve(input_list, window, 'valid')\n",
    "    plt.plot(range(len(ave_values)), ave_values)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160a3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # neural network modules\n",
    "import torch.nn.functional as F  # activation functions\n",
    "import torch.optim as optim  # optimizer\n",
    "from torch.autograd import Variable # add gradients to tensors\n",
    "from torch.nn import Parameter # model parameter functionality\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainQlearning(model: nn.Module, env: MultiActorEnv, loss: torch.nn.modules.loss = nn.MSELoss()):\n",
    "    '''\n",
    "    This function trains the model using the Q-learning algorithm\n",
    "    '''\n",
    "    lr = 0.001\n",
    "    num_of_play, max_play_length = 1000, 50\n",
    "    gamma, epsilon, epsilon_low, epsilon_step = 0.99, 0.5, 0.1, 0.05\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_func = loss\n",
    "\n",
    "    init_seed()\n",
    "    model.apply(init_weights)\n",
    "    model.train()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for i in range(1, num_of_play+1):\n",
    "        done = False\n",
    "        counter = 0\n",
    "        env.reset()\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            print(\"play round: {}, ave reward (last {}): {:.4f}\".format(\n",
    "                i, 100, sum(rewards[-100:])/100))\n",
    "            if epsilon > epsilon_low:\n",
    "                epsilon -= epsilon_step\n",
    "        while (counter <= max_play_length):\n",
    "            counter += 1\n",
    "            actor_number = env.get_actor_number()\n",
    "            if (actor_number==0): break\n",
    "            \n",
    "            inputs = []            \n",
    "            choices = []\n",
    "            for actor_id in range(actor_number):\n",
    "                # find Q(s_{t},a) for all actions\n",
    "                preds = []\n",
    "                state = env.get_state(actor_id)\n",
    "                for action in env.actions:\n",
    "                    pred = model(state, action)\n",
    "                    preds.append(pred)\n",
    "\n",
    "                p = np.random.uniform(0, 1)\n",
    "                choice = -1\n",
    "                if p < epsilon:\n",
    "                    choice = np.random.randint(0, 4)\n",
    "                else:\n",
    "                    list_pred = [x.item() for x in preds]\n",
    "                    max_pred = np.amax(list_pred)\n",
    "                    max_positions = np.argwhere(\n",
    "                        list_pred == max_pred).flatten().tolist()\n",
    "                    choice = random.choice(max_positions)\n",
    "                \n",
    "                choices.append(choice)\n",
    "                inputs.append(preds[choice])\n",
    "                \n",
    "            # take the action, s_{t},a -> s_{t+1}\n",
    "            # get the immediate reward\n",
    "            imm_reward, dones = env.step(choices)\n",
    "            \n",
    "            targets = []\n",
    "            for actor_id in range(actor_number):\n",
    "                future_reward = 0\n",
    "                # find Q(s_{t+1},a) for all actions\n",
    "                if not dones[actor_id]:\n",
    "                    with torch.no_grad():\n",
    "                        next_preds = []\n",
    "                        state = env.get_state(actor_id)\n",
    "                        for action in env.actions:\n",
    "                            next_pred = model(state, action)\n",
    "                            next_preds.append(next_pred.item())\n",
    "                        future_reward = max(next_preds)\n",
    "                        \n",
    "                # Q(s,a|t) = r + gamma*max[Q(s,a|t+1)]\n",
    "                tot_reward = imm_reward[actor_id] + gamma * future_reward\n",
    "                rewards.append(tot_reward)\n",
    "                targets.append(torch.Tensor([tot_reward]))\n",
    "\n",
    "            inputs = torch.stack(inputs, dim=0)\n",
    "            targets = torch.stack(targets, dim=0)\n",
    "            #print(inputs)\n",
    "            #print(targets)\n",
    "            #break\n",
    "            loss = loss_func(inputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            env.remove_dones()\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeed0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = TrainQlearning(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbcb6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MovingAveragePlot(rewards, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fa064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_movs(env, model):\n",
    "    movs = [[],[]]\n",
    "    actor_number = env.get_actor_number()\n",
    "    if (actor_number==0): return movs\n",
    "    for actor_id in range(actor_number):\n",
    "        preds = []\n",
    "        state = env.get_state(actor_id)\n",
    "        for action in env.actions:\n",
    "            pred = model(state, action)\n",
    "            preds.append(pred)\n",
    "        choice = -1\n",
    "        list_pred = [x.item() for x in preds]\n",
    "        max_pred = np.amax(list_pred)\n",
    "        max_positions = np.argwhere(list_pred == max_pred).flatten().tolist()\n",
    "        choice = random.choice(max_positions)\n",
    "        choices.append(choice)\n",
    "        if choice == 0:\n",
    "            movs[0].append(0)\n",
    "            movs[1].append(-1)\n",
    "        elif choice == 1:\n",
    "            movs[0].append(0)\n",
    "            movs[1].append(1)\n",
    "        elif choice == 2:\n",
    "            movs[0].append(-1)\n",
    "            movs[1].append(0)\n",
    "        elif choice == 3:\n",
    "            movs[0].append(1)\n",
    "            movs[1].append(0)\n",
    "        elif choice == 4:\n",
    "            movs[0].append(0)\n",
    "            movs[1].append(0)\n",
    "        else:\n",
    "            assert False, \"Unknow operation\"\n",
    "    return movs, choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.grid)\n",
    "env.display()\n",
    "print(env.get_actor_number())\n",
    "print(\"-------------------------------------------------------------\")\n",
    "counter = 0\n",
    "max_play_length = 50\n",
    "model.eval()\n",
    "while (counter <= max_play_length and env.get_actor_number() > 0):\n",
    "    # find Q(s_{t},a) for all actions\n",
    "    \n",
    "    counter += 1\n",
    "    actor_number = env.get_actor_number()\n",
    "    if (actor_number==0): break\n",
    "    \n",
    "    choices = []\n",
    "    for actor_id in range(actor_number):\n",
    "        preds = []\n",
    "        state = env.get_state(actor_id)\n",
    "        for action in env.actions:\n",
    "            pred = model(state, action)\n",
    "            preds.append(pred)\n",
    "        choice = -1\n",
    "        list_pred = [x.item() for x in preds]\n",
    "        print(list_pred)\n",
    "        max_pred = np.amax(list_pred)\n",
    "        max_positions = np.argwhere(list_pred == max_pred).flatten().tolist()\n",
    "        choice = random.choice(max_positions)\n",
    "        choices.append(choice)\n",
    "    print(choices)\n",
    "    env.step(choices)\n",
    "    print(env.grid)\n",
    "    env.display()\n",
    "    env.remove_dones()\n",
    "    print(env.get_actor_number())\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3020466",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.grid)\n",
    "env.display()\n",
    "\n",
    "env.step(['r', 'l', 'l', 'r'])\n",
    "print(env.grid)\n",
    "env.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoutePlot(width, height, obs, tgts, locs, movs=None, image_name='foo.png'):\n",
    "    plt.figure(figsize=(width, height))\n",
    "    \n",
    "    plt.xlim([-0.5, width-0.5])\n",
    "    plt.ylim([-0.5, height-0.5])\n",
    "    x_major_ticks = np.arange(-0.5, width+0.5, 1)\n",
    "    y_major_ticks = np.arange(-0.5, height+0.5, 1)\n",
    "    plt.xticks(x_major_ticks)\n",
    "    plt.yticks(y_major_ticks)\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    ax.axes.xaxis.set_ticklabels([])\n",
    "    ax.axes.yaxis.set_ticklabels([])\n",
    "    ax.tick_params(axis=\"x\",direction=\"in\")\n",
    "    ax.tick_params(axis=\"y\",direction=\"in\")\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    # show obs\n",
    "    plt.scatter(obs[0], obs[1], s=500, marker='x', color='r')\n",
    "    \n",
    "    # show tgts\n",
    "    plt.scatter(tgts[0], tgts[1], s=1000, marker='o', color=(0.5, 0.5, 1.0, 0.75))\n",
    "    for x,y,l in zip(tgts[0], tgts[1], tgts[2]):\n",
    "        label = \"{}\".format(l)\n",
    "        plt.annotate(label, (x,y), size=20, textcoords=\"offset points\", xytext=(0,-7), ha='center')\n",
    "    \n",
    "    # show mov direction\n",
    "    if movs is not None:\n",
    "        for i in range(len(locs[0])-len(movs[0])):\n",
    "            movs[0].append(0)\n",
    "            movs[1].append(0)\n",
    "        plt.quiver(locs[0], locs[1], movs[0], movs[1], color=(0, 0.2, 0), angles='xy', scale_units='xy', scale=1)\n",
    "    \n",
    "    # show locs\n",
    "    plt.scatter(locs[0], locs[1], s=1000, marker='o', color=(0.0, 0.5, 0.0))\n",
    "    for x,y,l in zip(locs[0], locs[1], locs[2]):\n",
    "        label = \"{}\".format(l)\n",
    "        plt.annotate(label, (x,y), size=20, textcoords=\"offset points\", xytext=(0,-7), ha='center')\n",
    "    \n",
    "    plt.savefig(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0485567",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 10\n",
    "height = 10\n",
    "env.reset()\n",
    "obs = env.get_obs()\n",
    "tgts = env.get_tgts()\n",
    "locs = env.get_locs()\n",
    "movs, _ = get_optimal_movs(env, model)\n",
    "RoutePlot(width, height, obs, tgts, locs, movs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1369590",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "width = 10\n",
    "height = 10\n",
    "\n",
    "counter = 0\n",
    "max_play_length = 50\n",
    "model.eval()\n",
    "while (counter <= max_play_length and env.get_actor_number() > 0):\n",
    "    # find Q(s_{t},a) for all actions\n",
    "    actor_number = env.get_actor_number()\n",
    "    if (actor_number==0): break\n",
    "    \n",
    "    choices = []\n",
    "    for actor_id in range(actor_number):\n",
    "        preds = []\n",
    "        state = env.get_state(actor_id)\n",
    "        for action in env.actions:\n",
    "            pred = model(state, action)\n",
    "            preds.append(pred)\n",
    "        choice = -1\n",
    "        list_pred = [x.item() for x in preds]\n",
    "        max_pred = np.amax(list_pred)\n",
    "        max_positions = np.argwhere(list_pred == max_pred).flatten().tolist()\n",
    "        choice = random.choice(max_positions)\n",
    "        choices.append(choice)\n",
    "        \n",
    "    \n",
    "    obs = env.get_obs()\n",
    "    tgts = env.get_tgts()\n",
    "    locs = env.get_locs()\n",
    "    movs, _ = get_optimal_movs(env, model)\n",
    "    RoutePlot(width, height, obs, tgts, locs, movs, str(counter)+\".png\")\n",
    "    \n",
    "    env.step(choices)\n",
    "    env.remove_dones()\n",
    "    counter += 1\n",
    "    \n",
    "obs = env.get_obs()\n",
    "tgts = env.get_tgts()\n",
    "locs = env.get_locs()\n",
    "RoutePlot(width, height, obs, tgts, locs, None, str(counter)+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43b3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
