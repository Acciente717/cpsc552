{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Codes\n",
    "\n",
    "This notebook contains example code of each module. Before committing this notebook, make sure all the output is cleared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Star Search Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AStar import AStar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2D matrix representing the map. 1 is obstacle. 0 is free block.\n",
    "grid = [\n",
    "    [0,1,1,0,0,0],\n",
    "    [0,1,0,0,1,0],\n",
    "    [0,1,1,0,1,0],\n",
    "    [0,0,0,0,1,0],\n",
    "    [1,1,1,1,1,0],\n",
    "    [1,1,1,1,1,0]\n",
    "]\n",
    "\n",
    "# goal is the lower right corner\n",
    "goal = (5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forbidden diagonal move.\n",
    "planner = AStar(grid, goal, False)\n",
    "\n",
    "# Find a path starting from the upper left corner.\n",
    "path = planner.plan(0, 0)\n",
    "\n",
    "print(path)\n",
    "len(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow diagonal move.\n",
    "planner = AStar(grid, goal, True)\n",
    "\n",
    "# Find a path starting from the upper left corner.\n",
    "path = planner.plan(0, 0)\n",
    "\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network with Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from PathPlanningEnv import PathPlanningEnv\n",
    "from FCNN import FCNN\n",
    "from Q_Network import Q_Network\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment and Network Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'height' : 5,\n",
    "    'width' : 10,\n",
    "    'obs_count' : 5,\n",
    "    'random_seed' : 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environments.\n",
    "env = PathPlanningEnv(**settings)\n",
    "env.display()\n",
    "print(env.distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create value network.\n",
    "network = FCNN(\n",
    "    input_dim = 3 * settings['height'] * settings['width'] + 4\n",
    ")\n",
    "\n",
    "# Or \n",
    "# network = Q_Network(\n",
    "#     BatchSize = 1, MapHeight = settings['height'], MapWidth = settings['width'],\n",
    "#     Covn1OutChan = 32, Conv1Kernel = 3, Covn2OutChan = 32, Conv2Kernel = 3, HiddenSize = 64\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.9\n",
    "epsilon_low = 0.1\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate)\n",
    "step_limit = 100\n",
    "num_of_play = 2000\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\n",
    "    torch.Tensor().new_tensor([1,0,0,0], dtype=torch.float32, requires_grad=False),\n",
    "    torch.Tensor().new_tensor([0,1,0,0], dtype=torch.float32, requires_grad=False),\n",
    "    torch.Tensor().new_tensor([0,0,1,0], dtype=torch.float32, requires_grad=False),\n",
    "    torch.Tensor().new_tensor([0,0,0,1], dtype=torch.float32, requires_grad=False)\n",
    "]\n",
    "\n",
    "rewards = []\n",
    "network.train()\n",
    "\n",
    "for i in range(1, num_of_play+1):\n",
    "    optimizer.zero_grad()\n",
    "    done = False\n",
    "    counter = 0\n",
    "    env.reset()\n",
    "    if i % 100 == 0:\n",
    "        print(\"play round: {}, ave reward (last {}): {}\".format(i, 100, sum(rewards[-100:])/100))\n",
    "        if epsilon > epsilon_low: epsilon -= 0.05\n",
    "\n",
    "    while counter <= step_limit and not done:\n",
    "        preds = []\n",
    "        \n",
    "        # copy one to avoid being modified at env.step()\n",
    "        state = env.grid.clone().detach().view(1, *env.grid.shape)\n",
    "        \n",
    "        # find Q(s_{t},a) for all actions\n",
    "        for action in actions:\n",
    "            pred = network(state, action)\n",
    "            preds.append(pred)\n",
    "\n",
    "        p = np.random.uniform(0,1)\n",
    "        if p < epsilon:\n",
    "            choice = np.random.randint(0,4)\n",
    "        else:\n",
    "            list_pred = [x.item() for x in preds]\n",
    "            max_pred = np.amax(list_pred)\n",
    "            max_positions = np.argwhere(list_pred == max_pred).flatten().tolist()\n",
    "            choice = random.choice(max_positions)\n",
    "            \n",
    "        # take the action, s_{t},a -> s_{t+1} and get the immediate reward\n",
    "        obs, imm_reward, done, _ = env.step(choice, early_stop=False, q_learning=True)\n",
    "        \n",
    "        # find Q(s_{t+1},a) for all actions\n",
    "        future_reward = 0\n",
    "        if not done:\n",
    "            next_preds = []\n",
    "            state = env.grid.clone().detach().view(1, *env.grid.shape)\n",
    "            for action in actions:\n",
    "                next_pred = network(state, action)\n",
    "                next_preds.append(next_pred.item())\n",
    "            future_reward = max(next_preds)\n",
    "        elif imm_reward == 1:\n",
    "            future_reward = 1\n",
    "        \n",
    "        tot_reward = imm_reward + gamma * future_reward\n",
    "        rewards.append(tot_reward)\n",
    "        counter += 1\n",
    "\n",
    "        # Q(s,a|t) = r + gamma*max[Q(s,a|t+1)]\n",
    "        loss = loss_func(preds[choice], torch.Tensor([tot_reward]))\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MovingAveragePlot(InputList, window_size):\n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    ave_values = np.convolve(InputList, window, 'same')\n",
    "    plt.plot(range(len(ave_values)), ave_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovingAveragePlot(rewards, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Path Finding Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlayOnce(env, network):\n",
    "    env.reset()\n",
    "    network.eval()\n",
    "\n",
    "    counter = 0\n",
    "    print(\"Time {}\".format(counter))\n",
    "    counter += 1\n",
    "    env.display()\n",
    "\n",
    "    done = False\n",
    "    while counter <= 100 and not done:\n",
    "        preds = []\n",
    "        state = env.grid.clone().detach().view(1, *env.grid.shape)\n",
    "        print(\"actions: \", end = \" \")\n",
    "        for action in actions:\n",
    "            pred = network(state, action)\n",
    "            preds.append(pred)\n",
    "            print(\"{:.8f}\".format(pred.item()), end = \" \")\n",
    "        print(\" \")\n",
    "\n",
    "        choice = np.argmax(np.array(preds))\n",
    "        obs, reward, done, _ = env.step(choice)\n",
    "\n",
    "        print(\"Time {}\".format(counter))\n",
    "        counter += 1\n",
    "        env.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test training case\n",
    "PlayOnce(env, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test other cases\n",
    "env = PathPlanningEnv(\n",
    "    grid = env.grid[2,:,:],\n",
    "    init_row = 4,\n",
    "    init_col = 9,\n",
    "    goal_row = env.goal_row,\n",
    "    goal_col = env.goal_col\n",
    ")\n",
    "PlayOnce(env, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
